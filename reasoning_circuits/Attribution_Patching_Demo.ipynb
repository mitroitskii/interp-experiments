{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sBhF_DtklnA"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Attribution_Patching_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iky3sdvklnN"
      },
      "source": [
        " # Attribution Patching Demo\n",
        " **Read [the accompanying blog post here](https://neelnanda.io/attribution-patching) for more context**\n",
        " This is an interim research report, giving a whirlwind tour of some unpublished work I did at Anthropic (credit to the then team - Chris Olah, Catherine Olsson, Nelson Elhage and Tristan Hume for help, support, and mentorship!)\n",
        "\n",
        " The goal of this work is run activation patching at an industrial scale, by using gradient based attribution to approximate the technique - allow an arbitrary number of patches to be made on two forwards and a single backward pass\n",
        "\n",
        " I have had less time than hoped to flesh out this investigation, but am writing up a rough investigation and comparison to standard activation patching on a few tasks to give a sense of the potential of this approach, and where it works vs falls down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fVipdG2klnP"
      },
      "source": [
        " <b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>\n",
        "\n",
        " **Tips for reading this Colab:**\n",
        " * You can run all this code for yourself!\n",
        " * The graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate\n",
        " * Collapse irrelevant sections with the dropdown arrows\n",
        " * Search the page using the search in the sidebar, not CTRL+F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFz23Wx1klnQ"
      },
      "source": [
        " ## Setup (Ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kDNzXuUklnQ",
        "outputId": "004224e4-9d69-4d63-e297-43b1130c7aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (1.6.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (3.5.1)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.43 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.51.3)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.13.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.19.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens) (3.11.15)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.43->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.43->transformer_lens) (0.21.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Requirement already satisfied: torchtyping in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from torchtyping) (2.6.0+cu124)\n",
            "Collecting typeguard<3,>=2.11.1 (from torchtyping)\n",
            "  Using cached typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (3.0.2)\n",
            "Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.2\n",
            "    Uninstalling typeguard-4.4.2:\n",
            "      Successfully uninstalled typeguard-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformer-lens 2.15.0 requires typeguard<5.0,>=4.2, but you have typeguard 2.13.3 which is incompatible.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typeguard-2.13.3\n",
            "Collecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
            "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-gqotlgxz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-gqotlgxz\n",
            "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (5.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->neel_plotly==0.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neel_plotly==0.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neel_plotly==0.0.0) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->neel_plotly==0.0.0) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->neel_plotly==0.0.0) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->neel_plotly==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->neel_plotly==0.0.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->neel_plotly==0.0.0) (3.0.2)\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.11/dist-packages (1.43.3)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from circuitsvis) (8.6.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from circuitsvis) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from circuitsvis) (2.6.0+cu124)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.1->circuitsvis) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.1->circuitsvis) (3.0.2)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.11/dist-packages (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEBUG_MODE = False\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install torchtyping\n",
        "    # Install my janky personal plotting utils\n",
        "    %pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
        "    # Install another version of node that makes PySvelte work way faster\n",
        "    %pip install circuitsvis\n",
        "    # Needed for PySvelte to work, v3 came out and broke things...\n",
        "    %pip install typeguard==2.13.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fT87LqNHklnT"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEBUG_MODE:\n",
        "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kxgOROepklnU"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtyping import TensorType as TT\n",
        "from typing import List, Union, Optional, Callable\n",
        "from functools import partial\n",
        "import copy\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZsejuWSOklnV"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import (\n",
        "    HookedTransformer,\n",
        "    HookedTransformerConfig,\n",
        "    FactoredMatrix,\n",
        "    ActivationCache,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqW4lIflklnW"
      },
      "source": [
        " Plotting helper functions from a janky personal library of plotting utils. The library is not documented and I recommend against trying to read it, just use your preferred plotting library if you want to do anything non-obvious:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5sRUEOgGklnX"
      },
      "outputs": [],
      "source": [
        "from neel_plotly import line, imshow, scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l7_Yi1x5klnY"
      },
      "outputs": [],
      "source": [
        "import transformer_lens.patching as patching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2LNSb8DklnY"
      },
      "source": [
        " ## IOI Patching Setup\n",
        " This just copies the relevant set up from Exploratory Analysis Demo, and isn't very important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAWdlBsdklnY",
        "outputId": "2f0767e3-c72a-4586-b148-5e7181232273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "\n",
            "\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
        "model.set_use_attn_result(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSmegXRCklnZ",
        "outputId": "09955a70-452d-4908-a7ac-210cff1d9c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean string 0 <|endoftext|>When John and Mary went to the shops, John gave the bag to\n",
            "Corrupted string 0 <|endoftext|>When John and Mary went to the shops, Mary gave the bag to\n",
            "Answer token indices tensor([[ 5335,  1757],\n",
            "        [ 1757,  5335],\n",
            "        [ 4186,  3700],\n",
            "        [ 3700,  4186],\n",
            "        [ 6035, 15686],\n",
            "        [15686,  6035],\n",
            "        [ 5780, 14235],\n",
            "        [14235,  5780]])\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"When John and Mary went to the shops, John gave the bag to\",\n",
        "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
        "    \"When Tom and James went to the park, James gave the ball to\",\n",
        "    \"When Tom and James went to the park, Tom gave the ball to\",\n",
        "    \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
        "    \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
        "    \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
        "    \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
        "]\n",
        "answers = [\n",
        "    (\" Mary\", \" John\"),\n",
        "    (\" John\", \" Mary\"),\n",
        "    (\" Tom\", \" James\"),\n",
        "    (\" James\", \" Tom\"),\n",
        "    (\" Dan\", \" Sid\"),\n",
        "    (\" Sid\", \" Dan\"),\n",
        "    (\" Martin\", \" Amy\"),\n",
        "    (\" Amy\", \" Martin\"),\n",
        "]\n",
        "\n",
        "clean_tokens = model.to_tokens(prompts)\n",
        "# Swap each adjacent pair, with a hacky list comprehension\n",
        "corrupted_tokens = clean_tokens[\n",
        "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
        "]\n",
        "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
        "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
        "\n",
        "answer_token_indices = torch.tensor(\n",
        "    [\n",
        "        [model.to_single_token(answers[i][j]) for j in range(2)]\n",
        "        for i in range(len(answers))\n",
        "    ],\n",
        "    device=model.cfg.device,\n",
        ")\n",
        "print(\"Answer token indices\", answer_token_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXetbwMuklnZ",
        "outputId": "feeb08d3-ebe2-4ac4-bdeb-28b26f6ddbbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean logit diff: 3.5519\n",
            "Corrupted logit diff: -3.5519\n"
          ]
        }
      ],
      "source": [
        "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
        "    if len(logits.shape) == 3:\n",
        "        # Get final logits only\n",
        "        logits = logits[:, -1, :]\n",
        "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
        "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
        "    return (correct_logits - incorrect_logits).mean()\n",
        "\n",
        "\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
        "\n",
        "clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
        "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
        "\n",
        "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
        "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k71jofYjklna",
        "outputId": "217ef7df-88bf-46f0-adcd-27271f7e8507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean Baseline is 1: 1.0000\n",
            "Corrupted Baseline is 0: 0.0000\n"
          ]
        }
      ],
      "source": [
        "CLEAN_BASELINE = clean_logit_diff\n",
        "CORRUPTED_BASELINE = corrupted_logit_diff\n",
        "\n",
        "\n",
        "def ioi_metric(logits, answer_token_indices=answer_token_indices):\n",
        "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
        "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
        "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikWqW0Fhklna"
      },
      "source": [
        " ## Patching\n",
        " In the following cells, we define attribution patching and use it in various ways on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9Li583PAklna"
      },
      "outputs": [],
      "source": [
        "Metric = Callable[[TT[\"batch_and_pos_dims\", \"d_model\"]], float]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "uDECnxOuklnb",
        "outputId": "1301aff6-f793-4896-9f0e-73c8b1e7e1ea"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Cannot add hook blocks.0.hook_attn_in if use_attn_in is False",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-92f4dd75af85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioi_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-12-92f4dd75af85>\u001b[0m in \u001b[0;36mget_cache_fwd_and_bwd\u001b[0;34m(model, tokens, metric)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_not_qkv_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_cache_hook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fwd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mgrad_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36madd_hook\u001b[0;34m(self, name, hook, dir, is_permanent, level, prepend)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook_point_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_point_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                     self.check_and_add_hook(\n\u001b[0m\u001b[1;32m    318\u001b[0m                         \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                         \u001b[0mhook_point_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mcheck_and_add_hook\u001b[0;34m(self, hook_point, hook_point_name, hook, dir, is_permanent, level, prepend)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;34m\"\"\"Runs checks on the hook, and then adds it to the hook point\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         self.check_hooks_to_add(\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mhook_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mhook_point_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mcheck_hooks_to_add\u001b[0;34m(self, hook_point, hook_point_name, hook, dir, is_permanent, prepend)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhook_point_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attn_in\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             assert (\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_attn_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             ), f\"Cannot add hook {hook_point_name} if use_attn_in is False\"\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Cannot add hook blocks.0.hook_attn_in if use_attn_in is False"
          ]
        }
      ],
      "source": [
        "filter_not_qkv_input = lambda name: \"_input\" not in name\n",
        "\n",
        "\n",
        "def get_cache_fwd_and_bwd(model, tokens, metric):\n",
        "    model.reset_hooks()\n",
        "    cache = {}\n",
        "\n",
        "    def forward_cache_hook(act, hook):\n",
        "        cache[hook.name] = act.detach()\n",
        "\n",
        "    model.add_hook(filter_not_qkv_input, forward_cache_hook, \"fwd\")\n",
        "\n",
        "    grad_cache = {}\n",
        "\n",
        "    def backward_cache_hook(act, hook):\n",
        "        grad_cache[hook.name] = act.detach()\n",
        "\n",
        "    model.add_hook(filter_not_qkv_input, backward_cache_hook, \"bwd\")\n",
        "\n",
        "    value = metric(model(tokens))\n",
        "    value.backward()\n",
        "    model.reset_hooks()\n",
        "    return (\n",
        "        value.item(),\n",
        "        ActivationCache(cache, model),\n",
        "        ActivationCache(grad_cache, model),\n",
        "    )\n",
        "\n",
        "\n",
        "clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(\n",
        "    model, clean_tokens, ioi_metric\n",
        ")\n",
        "print(\"Clean Value:\", clean_value)\n",
        "print(\"Clean Activations Cached:\", len(clean_cache))\n",
        "print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
        "corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(\n",
        "    model, corrupted_tokens, ioi_metric\n",
        ")\n",
        "print(\"Corrupted Value:\", corrupted_value)\n",
        "print(\"Corrupted Activations Cached:\", len(corrupted_cache))\n",
        "print(\"Corrupted Gradients Cached:\", len(corrupted_grad_cache))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA67ZwJEklnb"
      },
      "source": [
        " ### Attention Attribution\n",
        " The easiest thing to start with is to not even engage with the corrupted tokens/patching, but to look at the attribution of the attention patterns - that is, the linear approximation to what happens if you set each element of the attention pattern to zero. This, as it turns out, is a good proxy to what is going on with each head!\n",
        " Note that this is *not* the same as what we will later do with patching. In particular, this does not set up a careful counterfactual! It's a good tool for what's generally going on in this problem, but does not control for eg stuff that systematically boosts John > Mary in general, stuff that says \"I should activate the IOI circuit\", etc. Though using logit diff as our metric *does*\n",
        " Each element of the batch is independent and the metric is an average logit diff, so we can analyse each batch element independently here. We'll look at the first one, and then at the average across the whole batch (note - 4 prompts have indirect object before subject, 4 prompts have it the other way round, making the average pattern harder to interpret - I plot it over the first sequence of tokens as a mildly misleading reference).\n",
        " We can compare it to the interpretability in the wild diagram, and basically instantly recover most of the circuit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwXBLBpJklnb"
      },
      "outputs": [],
      "source": [
        "def create_attention_attr(\n",
        "    clean_cache, clean_grad_cache\n",
        ") -> TT[\"batch\", \"layer\", \"head_index\", \"dest\", \"src\"]:\n",
        "    attention_stack = torch.stack(\n",
        "        [clean_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
        "    )\n",
        "    attention_grad_stack = torch.stack(\n",
        "        [clean_grad_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
        "    )\n",
        "    attention_attr = attention_grad_stack * attention_stack\n",
        "    attention_attr = einops.rearrange(\n",
        "        attention_attr,\n",
        "        \"layer batch head_index dest src -> batch layer head_index dest src\",\n",
        "    )\n",
        "    return attention_attr\n",
        "\n",
        "\n",
        "attention_attr = create_attention_attr(clean_cache, clean_grad_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvtHS97Bklnc"
      },
      "outputs": [],
      "source": [
        "HEAD_NAMES = [\n",
        "    f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)\n",
        "]\n",
        "HEAD_NAMES_SIGNED = [f\"{name}{sign}\" for name in HEAD_NAMES for sign in [\"+\", \"-\"]]\n",
        "HEAD_NAMES_QKV = [\n",
        "    f\"{name}{act_name}\" for name in HEAD_NAMES for act_name in [\"Q\", \"K\", \"V\"]\n",
        "]\n",
        "print(HEAD_NAMES[:5])\n",
        "print(HEAD_NAMES_SIGNED[:5])\n",
        "print(HEAD_NAMES_QKV[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhn2I5Y3klnc"
      },
      "source": [
        " An extremely janky way to plot the attention attribution patterns. We scale them to be in [-1, 1], split each head into a positive and negative part (so all of it is in [0, 1]), and then plot the top 20 head-halves (a head can appear twice!) by the max value of the attribution pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeuKI5K6klnd"
      },
      "outputs": [],
      "source": [
        "def plot_attention_attr(attention_attr, tokens, top_k=20, index=0, title=\"\"):\n",
        "    if len(tokens.shape) == 2:\n",
        "        tokens = tokens[index]\n",
        "    if len(attention_attr.shape) == 5:\n",
        "        attention_attr = attention_attr[index]\n",
        "    attention_attr_pos = attention_attr.clamp(min=-1e-5)\n",
        "    attention_attr_neg = -attention_attr.clamp(max=1e-5)\n",
        "    attention_attr_signed = torch.stack([attention_attr_pos, attention_attr_neg], dim=0)\n",
        "    attention_attr_signed = einops.rearrange(\n",
        "        attention_attr_signed,\n",
        "        \"sign layer head_index dest src -> (layer head_index sign) dest src\",\n",
        "    )\n",
        "    attention_attr_signed = attention_attr_signed / attention_attr_signed.max()\n",
        "    attention_attr_indices = (\n",
        "        attention_attr_signed.max(-1).values.max(-1).values.argsort(descending=True)\n",
        "    )\n",
        "    # print(attention_attr_indices.shape)\n",
        "    # print(attention_attr_indices)\n",
        "    attention_attr_signed = attention_attr_signed[attention_attr_indices, :, :]\n",
        "    head_labels = [HEAD_NAMES_SIGNED[i.item()] for i in attention_attr_indices]\n",
        "\n",
        "    if title:\n",
        "        display(Markdown(\"### \" + title))\n",
        "    display(\n",
        "        pysvelte.AttentionMulti(\n",
        "            tokens=model.to_str_tokens(tokens),\n",
        "            attention=attention_attr_signed.permute(1, 2, 0)[:, :, :top_k],\n",
        "            head_labels=head_labels[:top_k],\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "plot_attention_attr(\n",
        "    attention_attr,\n",
        "    clean_tokens,\n",
        "    index=0,\n",
        "    title=\"Attention Attribution for first sequence\",\n",
        ")\n",
        "\n",
        "plot_attention_attr(\n",
        "    attention_attr.sum(0),\n",
        "    clean_tokens[0],\n",
        "    title=\"Summed Attention Attribution for all sequences\",\n",
        ")\n",
        "print(\n",
        "    \"Note: Plotted over first sequence for reference, but pairs have IO and S1 in different positions.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY3UwYedklne"
      },
      "source": [
        " ## Attribution Patching\n",
        " In the following sections, I will implement various kinds of attribution patching, and then compare them to the activation patching patterns (activation patching code copied from [Exploratory Analysis Demo](https://neelnanda.io/exploratory-analysis-demo))\n",
        " ### Residual Stream Patching\n",
        " <details><summary>Note: We add up across both d_model and batch (Explanation).</summary>\n",
        " We add up along d_model because we're taking the dot product - the derivative *is* the linear map that locally linearly approximates the metric, and so we take the dot product of our change vector with the derivative vector. Equivalent, we look at the effect of changing each coordinate independently, and then combine them by adding it up - it's linear, so this totally works.\n",
        " We add up across batch because we're taking the average of the metric, so each individual batch element provides `1/batch_size` of the overall effect. Because each batch element is independent of the others and no information moves between activations for different inputs, the batched version is equivalent to doing attribution patching separately for each input, and then averaging - in this second version the metric per input is *not* divided by batch_size because we don't average.</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fuMUJUBklne"
      },
      "outputs": [],
      "source": [
        "def attr_patch_residual(\n",
        "    clean_cache: ActivationCache,\n",
        "    corrupted_cache: ActivationCache,\n",
        "    corrupted_grad_cache: ActivationCache,\n",
        ") -> TT[\"component\", \"pos\"]:\n",
        "    clean_residual, residual_labels = clean_cache.accumulated_resid(\n",
        "        -1, incl_mid=True, return_labels=True\n",
        "    )\n",
        "    corrupted_residual = corrupted_cache.accumulated_resid(\n",
        "        -1, incl_mid=True, return_labels=False\n",
        "    )\n",
        "    corrupted_grad_residual = corrupted_grad_cache.accumulated_resid(\n",
        "        -1, incl_mid=True, return_labels=False\n",
        "    )\n",
        "    residual_attr = einops.reduce(\n",
        "        corrupted_grad_residual * (clean_residual - corrupted_residual),\n",
        "        \"component batch pos d_model -> component pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    return residual_attr, residual_labels\n",
        "\n",
        "\n",
        "residual_attr, residual_labels = attr_patch_residual(\n",
        "    clean_cache, corrupted_cache, corrupted_grad_cache\n",
        ")\n",
        "imshow(\n",
        "    residual_attr,\n",
        "    y=residual_labels,\n",
        "    yaxis=\"Component\",\n",
        "    xaxis=\"Position\",\n",
        "    title=\"Residual Attribution Patching\",\n",
        ")\n",
        "\n",
        "# ### Layer Output Patching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlMH8Bljklnf"
      },
      "outputs": [],
      "source": [
        "def attr_patch_layer_out(\n",
        "    clean_cache: ActivationCache,\n",
        "    corrupted_cache: ActivationCache,\n",
        "    corrupted_grad_cache: ActivationCache,\n",
        ") -> TT[\"component\", \"pos\"]:\n",
        "    clean_layer_out, labels = clean_cache.decompose_resid(-1, return_labels=True)\n",
        "    corrupted_layer_out = corrupted_cache.decompose_resid(-1, return_labels=False)\n",
        "    corrupted_grad_layer_out = corrupted_grad_cache.decompose_resid(\n",
        "        -1, return_labels=False\n",
        "    )\n",
        "    layer_out_attr = einops.reduce(\n",
        "        corrupted_grad_layer_out * (clean_layer_out - corrupted_layer_out),\n",
        "        \"component batch pos d_model -> component pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    return layer_out_attr, labels\n",
        "\n",
        "\n",
        "layer_out_attr, layer_out_labels = attr_patch_layer_out(\n",
        "    clean_cache, corrupted_cache, corrupted_grad_cache\n",
        ")\n",
        "imshow(\n",
        "    layer_out_attr,\n",
        "    y=layer_out_labels,\n",
        "    yaxis=\"Component\",\n",
        "    xaxis=\"Position\",\n",
        "    title=\"Layer Output Attribution Patching\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MomrHjv6klnf"
      },
      "outputs": [],
      "source": [
        "def attr_patch_head_out(\n",
        "    clean_cache: ActivationCache,\n",
        "    corrupted_cache: ActivationCache,\n",
        "    corrupted_grad_cache: ActivationCache,\n",
        ") -> TT[\"component\", \"pos\"]:\n",
        "    labels = HEAD_NAMES\n",
        "\n",
        "    clean_head_out = clean_cache.stack_head_results(-1, return_labels=False)\n",
        "    corrupted_head_out = corrupted_cache.stack_head_results(-1, return_labels=False)\n",
        "    corrupted_grad_head_out = corrupted_grad_cache.stack_head_results(\n",
        "        -1, return_labels=False\n",
        "    )\n",
        "    head_out_attr = einops.reduce(\n",
        "        corrupted_grad_head_out * (clean_head_out - corrupted_head_out),\n",
        "        \"component batch pos d_model -> component pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    return head_out_attr, labels\n",
        "\n",
        "\n",
        "head_out_attr, head_out_labels = attr_patch_head_out(\n",
        "    clean_cache, corrupted_cache, corrupted_grad_cache\n",
        ")\n",
        "imshow(\n",
        "    head_out_attr,\n",
        "    y=head_out_labels,\n",
        "    yaxis=\"Component\",\n",
        "    xaxis=\"Position\",\n",
        "    title=\"Head Output Attribution Patching\",\n",
        ")\n",
        "sum_head_out_attr = einops.reduce(\n",
        "    head_out_attr,\n",
        "    \"(layer head) pos -> layer head\",\n",
        "    \"sum\",\n",
        "    layer=model.cfg.n_layers,\n",
        "    head=model.cfg.n_heads,\n",
        ")\n",
        "imshow(\n",
        "    sum_head_out_attr,\n",
        "    yaxis=\"Layer\",\n",
        "    xaxis=\"Head Index\",\n",
        "    title=\"Head Output Attribution Patching Sum Over Pos\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5W5llvGklng"
      },
      "source": [
        " ### Head Activation Patching\n",
        " Intuitively, a head has three inputs, keys, queries and values. We can patch each of these individually to get a sense for where the important part of each head's input comes from!\n",
        " As a sanity check, we also do this for the mixed value. The result is a linear map of this (`z @ W_O == result`), so this is the same as patching the output of the head.\n",
        " We plot both the patch for each head over each position, and summed over position (it tends to be pretty sparse, so the latter is the same)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoyeeyj4klng"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Literal\n",
        "\n",
        "\n",
        "def stack_head_vector_from_cache(\n",
        "    cache, activation_name: Literal[\"q\", \"k\", \"v\", \"z\"]\n",
        ") -> TT[\"layer_and_head_index\", \"batch\", \"pos\", \"d_head\"]:\n",
        "    \"\"\"Stacks the head vectors from the cache from a specific activation (key, query, value or mixed_value (z)) into a single tensor.\"\"\"\n",
        "    stacked_head_vectors = torch.stack(\n",
        "        [cache[activation_name, l] for l in range(model.cfg.n_layers)], dim=0\n",
        "    )\n",
        "    stacked_head_vectors = einops.rearrange(\n",
        "        stacked_head_vectors,\n",
        "        \"layer batch pos head_index d_head -> (layer head_index) batch pos d_head\",\n",
        "    )\n",
        "    return stacked_head_vectors\n",
        "\n",
        "\n",
        "def attr_patch_head_vector(\n",
        "    clean_cache: ActivationCache,\n",
        "    corrupted_cache: ActivationCache,\n",
        "    corrupted_grad_cache: ActivationCache,\n",
        "    activation_name: Literal[\"q\", \"k\", \"v\", \"z\"],\n",
        ") -> TT[\"component\", \"pos\"]:\n",
        "    labels = HEAD_NAMES\n",
        "\n",
        "    clean_head_vector = stack_head_vector_from_cache(clean_cache, activation_name)\n",
        "    corrupted_head_vector = stack_head_vector_from_cache(\n",
        "        corrupted_cache, activation_name\n",
        "    )\n",
        "    corrupted_grad_head_vector = stack_head_vector_from_cache(\n",
        "        corrupted_grad_cache, activation_name\n",
        "    )\n",
        "    head_vector_attr = einops.reduce(\n",
        "        corrupted_grad_head_vector * (clean_head_vector - corrupted_head_vector),\n",
        "        \"component batch pos d_head -> component pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    return head_vector_attr, labels\n",
        "\n",
        "\n",
        "head_vector_attr_dict = {}\n",
        "for activation_name, activation_name_full in [\n",
        "    (\"k\", \"Key\"),\n",
        "    (\"q\", \"Query\"),\n",
        "    (\"v\", \"Value\"),\n",
        "    (\"z\", \"Mixed Value\"),\n",
        "]:\n",
        "    display(Markdown(f\"#### {activation_name_full} Head Vector Attribution Patching\"))\n",
        "    head_vector_attr_dict[activation_name], head_vector_labels = attr_patch_head_vector(\n",
        "        clean_cache, corrupted_cache, corrupted_grad_cache, activation_name\n",
        "    )\n",
        "    imshow(\n",
        "        head_vector_attr_dict[activation_name],\n",
        "        y=head_vector_labels,\n",
        "        yaxis=\"Component\",\n",
        "        xaxis=\"Position\",\n",
        "        title=f\"{activation_name_full} Attribution Patching\",\n",
        "    )\n",
        "    sum_head_vector_attr = einops.reduce(\n",
        "        head_vector_attr_dict[activation_name],\n",
        "        \"(layer head) pos -> layer head\",\n",
        "        \"sum\",\n",
        "        layer=model.cfg.n_layers,\n",
        "        head=model.cfg.n_heads,\n",
        "    )\n",
        "    imshow(\n",
        "        sum_head_vector_attr,\n",
        "        yaxis=\"Layer\",\n",
        "        xaxis=\"Head Index\",\n",
        "        title=f\"{activation_name_full} Attribution Patching Sum Over Pos\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSUnOcT-klng"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Literal\n",
        "\n",
        "\n",
        "def stack_head_pattern_from_cache(\n",
        "    cache,\n",
        ") -> TT[\"layer_and_head_index\", \"batch\", \"dest_pos\", \"src_pos\"]:\n",
        "    \"\"\"Stacks the head patterns from the cache into a single tensor.\"\"\"\n",
        "    stacked_head_pattern = torch.stack(\n",
        "        [cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
        "    )\n",
        "    stacked_head_pattern = einops.rearrange(\n",
        "        stacked_head_pattern,\n",
        "        \"layer batch head_index dest_pos src_pos -> (layer head_index) batch dest_pos src_pos\",\n",
        "    )\n",
        "    return stacked_head_pattern\n",
        "\n",
        "\n",
        "def attr_patch_head_pattern(\n",
        "    clean_cache: ActivationCache,\n",
        "    corrupted_cache: ActivationCache,\n",
        "    corrupted_grad_cache: ActivationCache,\n",
        ") -> TT[\"component\", \"dest_pos\", \"src_pos\"]:\n",
        "    labels = HEAD_NAMES\n",
        "\n",
        "    clean_head_pattern = stack_head_pattern_from_cache(clean_cache)\n",
        "    corrupted_head_pattern = stack_head_pattern_from_cache(corrupted_cache)\n",
        "    corrupted_grad_head_pattern = stack_head_pattern_from_cache(corrupted_grad_cache)\n",
        "    head_pattern_attr = einops.reduce(\n",
        "        corrupted_grad_head_pattern * (clean_head_pattern - corrupted_head_pattern),\n",
        "        \"component batch dest_pos src_pos -> component dest_pos src_pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    return head_pattern_attr, labels\n",
        "\n",
        "\n",
        "head_pattern_attr, labels = attr_patch_head_pattern(\n",
        "    clean_cache, corrupted_cache, corrupted_grad_cache\n",
        ")\n",
        "\n",
        "plot_attention_attr(\n",
        "    einops.rearrange(\n",
        "        head_pattern_attr,\n",
        "        \"(layer head) dest src -> layer head dest src\",\n",
        "        layer=model.cfg.n_layers,\n",
        "        head=model.cfg.n_heads,\n",
        "    ),\n",
        "    clean_tokens,\n",
        "    index=0,\n",
        "    title=\"Head Pattern Attribution Patching\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqSi7CWZklnj"
      },
      "outputs": [],
      "source": [
        "def get_head_vector_grad_input_from_grad_cache(\n",
        "    grad_cache: ActivationCache, activation_name: Literal[\"q\", \"k\", \"v\"], layer: int\n",
        ") -> TT[\"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
        "    vector_grad = grad_cache[activation_name, layer]\n",
        "    ln_scales = grad_cache[\"scale\", layer, \"ln1\"]\n",
        "    attn_layer_object = model.blocks[layer].attn\n",
        "    if activation_name == \"q\":\n",
        "        W = attn_layer_object.W_Q\n",
        "    elif activation_name == \"k\":\n",
        "        W = attn_layer_object.W_K\n",
        "    elif activation_name == \"v\":\n",
        "        W = attn_layer_object.W_V\n",
        "    else:\n",
        "        raise ValueError(\"Invalid activation name\")\n",
        "\n",
        "    return einsum(\n",
        "        \"batch pos head_index d_head, batch pos, head_index d_model d_head -> batch pos head_index d_model\",\n",
        "        vector_grad,\n",
        "        ln_scales.squeeze(-1),\n",
        "        W,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_stacked_head_vector_grad_input(\n",
        "    grad_cache, activation_name: Literal[\"q\", \"k\", \"v\"]\n",
        ") -> TT[\"layer\", \"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
        "    return torch.stack(\n",
        "        [\n",
        "            get_head_vector_grad_input_from_grad_cache(grad_cache, activation_name, l)\n",
        "            for l in range(model.cfg.n_layers)\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_full_vector_grad_input(\n",
        "    grad_cache,\n",
        ") -> TT[\"qkv\", \"layer\", \"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
        "    return torch.stack(\n",
        "        [\n",
        "            get_stacked_head_vector_grad_input(grad_cache, activation_name)\n",
        "            for activation_name in [\"q\", \"k\", \"v\"]\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "\n",
        "def attr_patch_head_path(\n",
        "    clean_cache: ActivationCache,\n",
        "    corrupted_cache: ActivationCache,\n",
        "    corrupted_grad_cache: ActivationCache,\n",
        ") -> TT[\"qkv\", \"dest_component\", \"src_component\", \"pos\"]:\n",
        "    \"\"\"\n",
        "    Computes the attribution patch along the path between each pair of heads.\n",
        "\n",
        "    Sets this to zero for the path from any late head to any early head\n",
        "\n",
        "    \"\"\"\n",
        "    start_labels = HEAD_NAMES\n",
        "    end_labels = HEAD_NAMES_QKV\n",
        "    full_vector_grad_input = get_full_vector_grad_input(corrupted_grad_cache)\n",
        "    clean_head_result_stack = clean_cache.stack_head_results(-1)\n",
        "    corrupted_head_result_stack = corrupted_cache.stack_head_results(-1)\n",
        "    diff_head_result = einops.rearrange(\n",
        "        clean_head_result_stack - corrupted_head_result_stack,\n",
        "        \"(layer head_index) batch pos d_model -> layer batch pos head_index d_model\",\n",
        "        layer=model.cfg.n_layers,\n",
        "        head_index=model.cfg.n_heads,\n",
        "    )\n",
        "    path_attr = einsum(\n",
        "        \"qkv layer_end batch pos head_end d_model, layer_start batch pos head_start d_model -> qkv layer_end head_end layer_start head_start pos\",\n",
        "        full_vector_grad_input,\n",
        "        diff_head_result,\n",
        "    )\n",
        "    correct_layer_order_mask = (\n",
        "        torch.arange(model.cfg.n_layers)[None, :, None, None, None, None]\n",
        "        > torch.arange(model.cfg.n_layers)[None, None, None, :, None, None]\n",
        "    ).to(path_attr.device)\n",
        "    zero = torch.zeros(1, device=path_attr.device)\n",
        "    path_attr = torch.where(correct_layer_order_mask, path_attr, zero)\n",
        "\n",
        "    path_attr = einops.rearrange(\n",
        "        path_attr,\n",
        "        \"qkv layer_end head_end layer_start head_start pos -> (layer_end head_end qkv) (layer_start head_start) pos\",\n",
        "    )\n",
        "    return path_attr, end_labels, start_labels\n",
        "\n",
        "\n",
        "head_path_attr, end_labels, start_labels = attr_patch_head_path(\n",
        "    clean_cache, corrupted_cache, corrupted_grad_cache\n",
        ")\n",
        "imshow(\n",
        "    head_path_attr.sum(-1),\n",
        "    y=end_labels,\n",
        "    yaxis=\"Path End (Head Input)\",\n",
        "    x=start_labels,\n",
        "    xaxis=\"Path Start (Head Output)\",\n",
        "    title=\"Head Path Attribution Patching\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtW0wxoXklnn"
      },
      "source": [
        " This is hard to parse. Here's an experiment with filtering for the most important heads and showing their paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B_4kz6Kklno"
      },
      "outputs": [],
      "source": [
        "head_out_values, head_out_indices = head_out_attr.sum(-1).abs().sort(descending=True)\n",
        "line(head_out_values)\n",
        "top_head_indices = head_out_indices[:22].sort().values\n",
        "top_end_indices = []\n",
        "top_end_labels = []\n",
        "top_start_indices = []\n",
        "top_start_labels = []\n",
        "for i in top_head_indices:\n",
        "    i = i.item()\n",
        "    top_start_indices.append(i)\n",
        "    top_start_labels.append(start_labels[i])\n",
        "    for j in range(3):\n",
        "        top_end_indices.append(3 * i + j)\n",
        "        top_end_labels.append(end_labels[3 * i + j])\n",
        "\n",
        "imshow(\n",
        "    head_path_attr[top_end_indices, :][:, top_start_indices].sum(-1),\n",
        "    y=top_end_labels,\n",
        "    yaxis=\"Path End (Head Input)\",\n",
        "    x=top_start_labels,\n",
        "    xaxis=\"Path Start (Head Output)\",\n",
        "    title=\"Head Path Attribution Patching (Filtered for Top Heads)\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_OlfJNTklno"
      },
      "outputs": [],
      "source": [
        "for j, composition_type in enumerate([\"Query\", \"Key\", \"Value\"]):\n",
        "    imshow(\n",
        "        head_path_attr[top_end_indices, :][:, top_start_indices][j::3].sum(-1),\n",
        "        y=top_end_labels[j::3],\n",
        "        yaxis=\"Path End (Head Input)\",\n",
        "        x=top_start_labels,\n",
        "        xaxis=\"Path Start (Head Output)\",\n",
        "        title=f\"Head Path to {composition_type} Attribution Patching (Filtered for Top Heads)\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGISzhc-klnr"
      },
      "outputs": [],
      "source": [
        "top_head_path_attr = einops.rearrange(\n",
        "    head_path_attr[top_end_indices, :][:, top_start_indices].sum(-1),\n",
        "    \"(head_end qkv) head_start -> qkv head_end head_start\",\n",
        "    qkv=3,\n",
        ")\n",
        "imshow(\n",
        "    top_head_path_attr,\n",
        "    y=[i[:-1] for i in top_end_labels[::3]],\n",
        "    yaxis=\"Path End (Head Input)\",\n",
        "    x=top_start_labels,\n",
        "    xaxis=\"Path Start (Head Output)\",\n",
        "    title=f\"Head Path Attribution Patching (Filtered for Top Heads)\",\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Query\", \"Key\", \"Value\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiFyZJKsklnr"
      },
      "source": [
        " Let's now dive into 3 interesting heads: L5H5 (induction head), L8H6 (S-Inhibition Head), L9H9 (Name Mover) and look at their input and output paths (note - Q input means )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWBt_tcEklnr"
      },
      "outputs": [],
      "source": [
        "interesting_heads = [\n",
        "    5 * model.cfg.n_heads + 5,\n",
        "    8 * model.cfg.n_heads + 6,\n",
        "    9 * model.cfg.n_heads + 9,\n",
        "]\n",
        "interesting_head_labels = [HEAD_NAMES[i] for i in interesting_heads]\n",
        "for head_index, label in zip(interesting_heads, interesting_head_labels):\n",
        "    in_paths = head_path_attr[3 * head_index : 3 * head_index + 3].sum(-1)\n",
        "    out_paths = head_path_attr[:, head_index].sum(-1)\n",
        "    out_paths = einops.rearrange(out_paths, \"(layer_head qkv) -> qkv layer_head\", qkv=3)\n",
        "    all_paths = torch.cat([in_paths, out_paths], dim=0)\n",
        "    all_paths = einops.rearrange(\n",
        "        all_paths,\n",
        "        \"path_type (layer head) -> path_type layer head\",\n",
        "        layer=model.cfg.n_layers,\n",
        "        head=model.cfg.n_heads,\n",
        "    )\n",
        "    imshow(\n",
        "        all_paths,\n",
        "        facet_col=0,\n",
        "        facet_labels=[\n",
        "            \"Query (In)\",\n",
        "            \"Key (In)\",\n",
        "            \"Value (In)\",\n",
        "            \"Query (Out)\",\n",
        "            \"Key (Out)\",\n",
        "            \"Value (Out)\",\n",
        "        ],\n",
        "        title=f\"Input and Output Paths for head {label}\",\n",
        "        yaxis=\"Layer\",\n",
        "        xaxis=\"Head\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wye0REJWklnr"
      },
      "source": [
        " ## Validating Attribution vs Activation Patching\n",
        " Let's now compare attribution and activation patching. Generally it's a decent approximation! The main place it fails is MLP0 and the residual stream\n",
        " My fuzzy intuition is that attribution patching works badly for \"big\" things which are poorly modelled as linear approximations, and works well for \"small\" things which are more like incremental changes. Anything involving replacing the embedding is a \"big\" thing, which includes residual streams, and in GPT-2 small MLP0 seems to be used as an \"extended embedding\" (where later layers use MLP0's output instead of the token embedding), so I also count it as big.\n",
        " See more discussion in the accompanying blog post!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFO0o231kln1"
      },
      "source": [
        " First do some refactoring to make attribution patching more generic. We make an attribution cache, which is an ActivationCache where each element is (clean_act - corrupted_act) * corrupted_grad, so that it's the per-element attribution for each activation. Thanks to linearity, we just compute things by adding stuff up along the relevant dimensions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaQM05W9kln1"
      },
      "outputs": [],
      "source": [
        "attribution_cache_dict = {}\n",
        "for key in corrupted_grad_cache.cache_dict.keys():\n",
        "    attribution_cache_dict[key] = corrupted_grad_cache.cache_dict[key] * (\n",
        "        clean_cache.cache_dict[key] - corrupted_cache.cache_dict[key]\n",
        "    )\n",
        "attr_cache = ActivationCache(attribution_cache_dict, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNHkoA1Wkln2"
      },
      "source": [
        " By block: For each head we patch the starting residual stream, attention output + MLP output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TXQJ4oQkln2"
      },
      "outputs": [],
      "source": [
        "str_tokens = model.to_str_tokens(clean_tokens[0])\n",
        "context_length = len(str_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcqsXCjQkln2"
      },
      "outputs": [],
      "source": [
        "every_block_act_patch_result = patching.get_act_patch_block_every(\n",
        "    model, corrupted_tokens, clean_cache, ioi_metric\n",
        ")\n",
        "imshow(\n",
        "    every_block_act_patch_result,\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"],\n",
        "    title=\"Activation Patching Per Block\",\n",
        "    xaxis=\"Position\",\n",
        "    yaxis=\"Layer\",\n",
        "    zmax=1,\n",
        "    zmin=-1,\n",
        "    x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBZwOMX4kln3"
      },
      "outputs": [],
      "source": [
        "def get_attr_patch_block_every(attr_cache):\n",
        "    resid_pre_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"resid_pre\"),\n",
        "        \"layer batch pos d_model -> layer pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    attn_out_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"attn_out\"),\n",
        "        \"layer batch pos d_model -> layer pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    mlp_out_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"mlp_out\"),\n",
        "        \"layer batch pos d_model -> layer pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "\n",
        "    every_block_attr_patch_result = torch.stack(\n",
        "        [resid_pre_attr, attn_out_attr, mlp_out_attr], dim=0\n",
        "    )\n",
        "    return every_block_attr_patch_result\n",
        "\n",
        "\n",
        "every_block_attr_patch_result = get_attr_patch_block_every(attr_cache)\n",
        "imshow(\n",
        "    every_block_attr_patch_result,\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"],\n",
        "    title=\"Attribution Patching Per Block\",\n",
        "    xaxis=\"Position\",\n",
        "    yaxis=\"Layer\",\n",
        "    zmax=1,\n",
        "    zmin=-1,\n",
        "    x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Y2gR_Ckln3"
      },
      "outputs": [],
      "source": [
        "scatter(\n",
        "    y=every_block_attr_patch_result.reshape(3, -1),\n",
        "    x=every_block_act_patch_result.reshape(3, -1),\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"],\n",
        "    title=\"Attribution vs Activation Patching Per Block\",\n",
        "    xaxis=\"Activation Patch\",\n",
        "    yaxis=\"Attribution Patch\",\n",
        "    hover=[\n",
        "        f\"Layer {l}, Position {p}, |{str_tokens[p]}|\"\n",
        "        for l in range(model.cfg.n_layers)\n",
        "        for p in range(context_length)\n",
        "    ],\n",
        "    color=einops.repeat(\n",
        "        torch.arange(model.cfg.n_layers), \"layer -> (layer pos)\", pos=context_length\n",
        "    ),\n",
        "    color_continuous_scale=\"Portland\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVFYtW9ykln4"
      },
      "source": [
        " By head: For each head we patch the output, query, key, value or pattern. We do all positions at once so it's not super slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXj0MsL2kln4"
      },
      "outputs": [],
      "source": [
        "every_head_all_pos_act_patch_result = patching.get_act_patch_attn_head_all_pos_every(\n",
        "    model, corrupted_tokens, clean_cache, ioi_metric\n",
        ")\n",
        "imshow(\n",
        "    every_head_all_pos_act_patch_result,\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
        "    title=\"Activation Patching Per Head (All Pos)\",\n",
        "    xaxis=\"Head\",\n",
        "    yaxis=\"Layer\",\n",
        "    zmax=1,\n",
        "    zmin=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gmaaEdmkln4"
      },
      "outputs": [],
      "source": [
        "def get_attr_patch_attn_head_all_pos_every(attr_cache):\n",
        "    head_out_all_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"z\"),\n",
        "        \"layer batch pos head_index d_head -> layer head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_q_all_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"q\"),\n",
        "        \"layer batch pos head_index d_head -> layer head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_k_all_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"k\"),\n",
        "        \"layer batch pos head_index d_head -> layer head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_v_all_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"v\"),\n",
        "        \"layer batch pos head_index d_head -> layer head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_pattern_all_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"pattern\"),\n",
        "        \"layer batch head_index dest_pos src_pos -> layer head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "\n",
        "    return torch.stack(\n",
        "        [\n",
        "            head_out_all_pos_attr,\n",
        "            head_q_all_pos_attr,\n",
        "            head_k_all_pos_attr,\n",
        "            head_v_all_pos_attr,\n",
        "            head_pattern_all_pos_attr,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "every_head_all_pos_attr_patch_result = get_attr_patch_attn_head_all_pos_every(\n",
        "    attr_cache\n",
        ")\n",
        "imshow(\n",
        "    every_head_all_pos_attr_patch_result,\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
        "    title=\"Attribution Patching Per Head (All Pos)\",\n",
        "    xaxis=\"Head\",\n",
        "    yaxis=\"Layer\",\n",
        "    zmax=1,\n",
        "    zmin=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UhDCJohkln4"
      },
      "outputs": [],
      "source": [
        "scatter(\n",
        "    y=every_head_all_pos_attr_patch_result.reshape(5, -1),\n",
        "    x=every_head_all_pos_act_patch_result.reshape(5, -1),\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
        "    title=\"Attribution vs Activation Patching Per Head (All Pos)\",\n",
        "    xaxis=\"Activation Patch\",\n",
        "    yaxis=\"Attribution Patch\",\n",
        "    include_diag=True,\n",
        "    hover=head_out_labels,\n",
        "    color=einops.repeat(\n",
        "        torch.arange(model.cfg.n_layers),\n",
        "        \"layer -> (layer head)\",\n",
        "        head=model.cfg.n_heads,\n",
        "    ),\n",
        "    color_continuous_scale=\"Portland\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkooTs-ckln4"
      },
      "source": [
        " We see pretty good results in general, but significant errors for heads L5H5 on query and moderate errors for head L10H7 on query and key, and moderate errors for head L11H10 on key. But each of these is fine for pattern and output. My guess is that the problem is that these have pretty saturated attention on a single token, and the linear approximation is thus not great on the attention calculation here, but I'm not sure. When we plot the attention patterns, we do see this!\n",
        " Note that the axis labels are for the *first* prompt's tokens, but each facet is a different prompt, so this is somewhat inaccurate. In particular, every odd facet has indirect object and subject in the opposite order (IO first). But otherwise everything lines up between the prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkNjVPWokln4"
      },
      "outputs": [],
      "source": [
        "graph_tok_labels = [\n",
        "    f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))\n",
        "]\n",
        "imshow(\n",
        "    clean_cache[\"pattern\", 5][:, 5],\n",
        "    x=graph_tok_labels,\n",
        "    y=graph_tok_labels,\n",
        "    facet_col=0,\n",
        "    title=\"Attention for Head L5H5\",\n",
        "    facet_name=\"Prompt\",\n",
        ")\n",
        "imshow(\n",
        "    clean_cache[\"pattern\", 10][:, 7],\n",
        "    x=graph_tok_labels,\n",
        "    y=graph_tok_labels,\n",
        "    facet_col=0,\n",
        "    title=\"Attention for Head L10H7\",\n",
        "    facet_name=\"Prompt\",\n",
        ")\n",
        "imshow(\n",
        "    clean_cache[\"pattern\", 11][:, 10],\n",
        "    x=graph_tok_labels,\n",
        "    y=graph_tok_labels,\n",
        "    facet_col=0,\n",
        "    title=\"Attention for Head L11H10\",\n",
        "    facet_name=\"Prompt\",\n",
        ")\n",
        "\n",
        "\n",
        "# [markdown]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vOcs3Qckln4"
      },
      "outputs": [],
      "source": [
        "every_head_by_pos_act_patch_result = patching.get_act_patch_attn_head_by_pos_every(\n",
        "    model, corrupted_tokens, clean_cache, ioi_metric\n",
        ")\n",
        "every_head_by_pos_act_patch_result = einops.rearrange(\n",
        "    every_head_by_pos_act_patch_result,\n",
        "    \"act_type layer pos head -> act_type (layer head) pos\",\n",
        ")\n",
        "imshow(\n",
        "    every_head_by_pos_act_patch_result,\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
        "    title=\"Activation Patching Per Head (By Pos)\",\n",
        "    xaxis=\"Position\",\n",
        "    yaxis=\"Layer & Head\",\n",
        "    zmax=1,\n",
        "    zmin=-1,\n",
        "    x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
        "    y=head_out_labels,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl5akvVpkln5"
      },
      "outputs": [],
      "source": [
        "def get_attr_patch_attn_head_by_pos_every(attr_cache):\n",
        "    head_out_by_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"z\"),\n",
        "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_q_by_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"q\"),\n",
        "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_k_by_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"k\"),\n",
        "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_v_by_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"v\"),\n",
        "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    head_pattern_by_pos_attr = einops.reduce(\n",
        "        attr_cache.stack_activation(\"pattern\"),\n",
        "        \"layer batch head_index dest_pos src_pos -> layer dest_pos head_index\",\n",
        "        \"sum\",\n",
        "    )\n",
        "\n",
        "    return torch.stack(\n",
        "        [\n",
        "            head_out_by_pos_attr,\n",
        "            head_q_by_pos_attr,\n",
        "            head_k_by_pos_attr,\n",
        "            head_v_by_pos_attr,\n",
        "            head_pattern_by_pos_attr,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "every_head_by_pos_attr_patch_result = get_attr_patch_attn_head_by_pos_every(attr_cache)\n",
        "every_head_by_pos_attr_patch_result = einops.rearrange(\n",
        "    every_head_by_pos_attr_patch_result,\n",
        "    \"act_type layer pos head -> act_type (layer head) pos\",\n",
        ")\n",
        "imshow(\n",
        "    every_head_by_pos_attr_patch_result,\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
        "    title=\"Attribution Patching Per Head (By Pos)\",\n",
        "    xaxis=\"Position\",\n",
        "    yaxis=\"Layer & Head\",\n",
        "    zmax=1,\n",
        "    zmin=-1,\n",
        "    x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
        "    y=head_out_labels,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb4ywe5zkln5"
      },
      "outputs": [],
      "source": [
        "scatter(\n",
        "    y=every_head_by_pos_attr_patch_result.reshape(5, -1),\n",
        "    x=every_head_by_pos_act_patch_result.reshape(5, -1),\n",
        "    facet_col=0,\n",
        "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
        "    title=\"Attribution vs Activation Patching Per Head (by Pos)\",\n",
        "    xaxis=\"Activation Patch\",\n",
        "    yaxis=\"Attribution Patch\",\n",
        "    include_diag=True,\n",
        "    hover=[f\"{label} {tok}\" for label in head_out_labels for tok in graph_tok_labels],\n",
        "    color=einops.repeat(\n",
        "        torch.arange(model.cfg.n_layers),\n",
        "        \"layer -> (layer head pos)\",\n",
        "        head=model.cfg.n_heads,\n",
        "        pos=15,\n",
        "    ),\n",
        "    color_continuous_scale=\"Portland\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E09VUCykln5"
      },
      "source": [
        " ## Factual Knowledge Patching Example\n",
        " Incomplete, but maybe of interest!\n",
        " Note that I have better results with the corrupted prompt as having random words rather than Colosseum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlOSaL6Ikln5"
      },
      "outputs": [],
      "source": [
        "gpt2_xl = HookedTransformer.from_pretrained(\"gpt2-xl\")\n",
        "clean_prompt = \"The Eiffel Tower is located in the city of\"\n",
        "clean_answer = \" Paris\"\n",
        "# corrupted_prompt = \"The red brown fox jumps is located in the city of\"\n",
        "corrupted_prompt = \"The Colosseum is located in the city of\"\n",
        "corrupted_answer = \" Rome\"\n",
        "utils.test_prompt(clean_prompt, clean_answer, gpt2_xl)\n",
        "utils.test_prompt(corrupted_prompt, corrupted_answer, gpt2_xl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufJ5WjXjkln5"
      },
      "outputs": [],
      "source": [
        "clean_answer_index = gpt2_xl.to_single_token(clean_answer)\n",
        "corrupted_answer_index = gpt2_xl.to_single_token(corrupted_answer)\n",
        "\n",
        "\n",
        "def factual_logit_diff(logits: TT[\"batch\", \"position\", \"d_vocab\"]):\n",
        "    return logits[0, -1, clean_answer_index] - logits[0, -1, corrupted_answer_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgq9xlNykln5"
      },
      "outputs": [],
      "source": [
        "clean_logits, clean_cache = gpt2_xl.run_with_cache(clean_prompt)\n",
        "CLEAN_LOGIT_DIFF_FACTUAL = factual_logit_diff(clean_logits).item()\n",
        "corrupted_logits, _ = gpt2_xl.run_with_cache(corrupted_prompt)\n",
        "CORRUPTED_LOGIT_DIFF_FACTUAL = factual_logit_diff(corrupted_logits).item()\n",
        "\n",
        "\n",
        "def factual_metric(logits: TT[\"batch\", \"position\", \"d_vocab\"]):\n",
        "    return (factual_logit_diff(logits) - CORRUPTED_LOGIT_DIFF_FACTUAL) / (\n",
        "        CLEAN_LOGIT_DIFF_FACTUAL - CORRUPTED_LOGIT_DIFF_FACTUAL\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Clean logit diff:\", CLEAN_LOGIT_DIFF_FACTUAL)\n",
        "print(\"Corrupted logit diff:\", CORRUPTED_LOGIT_DIFF_FACTUAL)\n",
        "print(\"Clean Metric:\", factual_metric(clean_logits))\n",
        "print(\"Corrupted Metric:\", factual_metric(corrupted_logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vogvUYP7kln5"
      },
      "outputs": [],
      "source": [
        "# corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(gpt2_xl, corrupted_prompt, factual_metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY_BVKAEkln5"
      },
      "outputs": [],
      "source": [
        "clean_tokens = gpt2_xl.to_tokens(clean_prompt)\n",
        "clean_str_tokens = gpt2_xl.to_str_tokens(clean_prompt)\n",
        "corrupted_tokens = gpt2_xl.to_tokens(corrupted_prompt)\n",
        "corrupted_str_tokens = gpt2_xl.to_str_tokens(corrupted_prompt)\n",
        "print(\"Clean:\", clean_str_tokens)\n",
        "print(\"Corrupted:\", corrupted_str_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkLpVRC0kln5"
      },
      "outputs": [],
      "source": [
        "def act_patch_residual(clean_cache, corrupted_tokens, model: HookedTransformer, metric):\n",
        "    if len(corrupted_tokens.shape) == 2:\n",
        "        corrupted_tokens = corrupted_tokens[0]\n",
        "    residual_patches = torch.zeros(\n",
        "        (model.cfg.n_layers, len(corrupted_tokens)), device=model.cfg.device\n",
        "    )\n",
        "\n",
        "    def residual_hook(resid_pre, hook, layer, pos):\n",
        "        resid_pre[:, pos, :] = clean_cache[\"resid_pre\", layer][:, pos, :]\n",
        "        return resid_pre\n",
        "\n",
        "    for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
        "        for pos in range(len(corrupted_tokens)):\n",
        "            patched_logits = model.run_with_hooks(\n",
        "                corrupted_tokens,\n",
        "                fwd_hooks=[\n",
        "                    (\n",
        "                        f\"blocks.{layer}.hook_resid_pre\",\n",
        "                        partial(residual_hook, layer=layer, pos=pos),\n",
        "                    )\n",
        "                ],\n",
        "            )\n",
        "            residual_patches[layer, pos] = metric(patched_logits).item()\n",
        "    return residual_patches\n",
        "\n",
        "\n",
        "residual_act_patch = act_patch_residual(\n",
        "    clean_cache, corrupted_tokens, gpt2_xl, factual_metric\n",
        ")\n",
        "\n",
        "imshow(\n",
        "    residual_act_patch,\n",
        "    title=\"Factual Recall Patching (Residual)\",\n",
        "    xaxis=\"Position\",\n",
        "    yaxis=\"Layer\",\n",
        "    x=clean_str_tokens,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
